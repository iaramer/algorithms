{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "reinforce_pytorch.solved.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UVkCC1iri2SN"
      },
      "source": [
        "# REINFORCE in PyTorch\n",
        "\n",
        "Just like we did before for Q-learning, this time we'll design a PyTorch network to learn `CartPole-v0` via policy gradient (REINFORCE).\n",
        "\n",
        "Most of the code in this notebook is taken from approximate Q-learning, so you'll find it more or less familiar and even simpler."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yjkAlO8Qi_Xm"
      },
      "source": [
        "Acknowledgements for this great notebook to the [Practical_RL](https://github.com/yandexdataschool/Practical_RL) course team."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1b0moDpxi2SW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6dda8c95-8807-4910-c090-52e752054d2b"
      },
      "source": [
        "import sys, os\n",
        "if 'google.colab' in sys.modules and not os.path.exists('.setup_complete'):\n",
        "    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/setup_colab.sh -O- | bash\n",
        "    !touch .setup_complete\n",
        "\n",
        "# This code creates a virtual display to draw game images on.\n",
        "# It will have no effect if your machine has a monitor.\n",
        "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:\n",
        "    !bash ../xvfb start\n",
        "    os.environ['DISPLAY'] = ':1'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selecting previously unselected package xvfb.\n",
            "(Reading database ... 155222 files and directories currently installed.)\n",
            "Preparing to unpack .../xvfb_2%3a1.19.6-1ubuntu4.9_amd64.deb ...\n",
            "Unpacking xvfb (2:1.19.6-1ubuntu4.9) ...\n",
            "Setting up xvfb (2:1.19.6-1ubuntu4.9) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Starting virtual X frame buffer: Xvfb.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7UYczVTli2Sb"
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98zhewLFi2Sd"
      },
      "source": [
        "A caveat: with some versions of `pyglet`, the following cell may crash with `NameError: name 'base' is not defined`. The corresponding bug report is [here](https://github.com/pyglet/pyglet/issues/134). If you see this error, try restarting the kernel."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XPKYrIlai2Sf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "outputId": "5b7ab3c7-9533-46e3-a344-59af08a348b6"
      },
      "source": [
        "env = gym.make(\"CartPole-v0\")\n",
        "\n",
        "# gym compatibility: unwrap TimeLimit\n",
        "if hasattr(env, '_max_episode_steps'):\n",
        "    env = env.env\n",
        "\n",
        "env.reset()\n",
        "n_actions = env.action_space.n\n",
        "state_dim = env.observation_space.shape\n",
        "\n",
        "plt.imshow(env.render(\"rgb_array\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fb494df7d90>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAATG0lEQVR4nO3de6xd5Znf8e/PF25JGnM5MR7bjMnEbURGxaBT4ij5gyFKBlBViJRE0IqgCMmDRKREitrAVOok0iDNKJ3QRp2SegQNmWRC6CQBFzFDGAdpFFVcTDCOwVycxBS7NjZXg5g4sf30j7NMNrYPZ5+bt9+zvx9pa6/1rLX2fl6x/WOd96x9VqoKSVI75g26AUnS5BjcktQYg1uSGmNwS1JjDG5JaozBLUmNmbXgTnJxkqeSbE1y/Wy9jyQNm8zGddxJ5gNPAx8DtgMPA1dW1RMz/maSNGRm64z7AmBrVf2iqn4N3A5cNkvvJUlDZcEsve5S4Lme9e3AB8fb+YwzzqgVK1bMUiuS1J5t27bxwgsv5GjbZiu4J5RkDbAG4KyzzmLDhg2DakWSjjujo6PjbputqZIdwPKe9WVd7U1VtbaqRqtqdGRkZJbakKS5Z7aC+2FgZZKzk5wAXAGsm6X3kqShMitTJVW1P8nngHuB+cCtVfX4bLyXJA2bWZvjrqp7gHtm6/UlaVj5zUlJaozBLUmNMbglqTEGtyQ1xuCWpMYY3JLUGINbkhpjcEtSYwxuSWqMwS1JjTG4JakxBrckNcbglqTGGNyS1BiDW5IaY3BLUmMMbklqjMEtSY2Z1q3LkmwDXgMOAPurajTJacD3gBXANuDTVfXy9NqUJB0yE2fcf1BVq6pqtFu/HlhfVSuB9d26JGmGzMZUyWXAbd3ybcDls/AekjS0phvcBfwoySNJ1nS1xVW1s1veBSye5ntIknpMa44b+EhV7UjyHuC+JE/2bqyqSlJHO7AL+jUAZ5111jTbkKThMa0z7qra0T3vBn4IXAA8n2QJQPe8e5xj11bVaFWNjoyMTKcNSRoqUw7uJO9I8q5Dy8DHgc3AOuDqbrergbum26Qk6bemM1WyGPhhkkOv8zdV9fdJHgbuSHIN8Czw6em3KUk6ZMrBXVW/AM49Sv1F4KPTaUqSND6/OSlJjTG4JakxBrckNcbglqTGGNyS1BiDW5IaY3BLUmMMbklqjMEtSY0xuCWpMQa3JDXG4JakxhjcktQYg1uSGmNwS1JjDG5JaozBLUmNMbglqTEGtyQ1ZsLgTnJrkt1JNvfUTktyX5JnuudTu3qSfD3J1iSbkpw/m81L0jDq54z7m8DFh9WuB9ZX1UpgfbcOcAmwsnusAW6emTYlSYdMGNxV9Y/AS4eVLwNu65ZvAy7vqX+rxjwALEqyZKaalSRNfY57cVXt7JZ3AYu75aXAcz37be9qR0iyJsmGJBv27NkzxTYkafhM+5eTVVVATeG4tVU1WlWjIyMj021DkobGVIP7+UNTIN3z7q6+A1jes9+yriZJmiFTDe51wNXd8tXAXT31z3RXl6wGXu2ZUpEkzYAFE+2Q5LvAhcAZSbYDfwL8GXBHkmuAZ4FPd7vfA1wKbAXeAD47Cz1L0lCbMLir6spxNn30KPsWcN10m5Ikjc9vTkpSYwxuSWqMwS1JjTG4JakxBrckNcbglqTGGNyS1BiDW5IaY3BLUmMMbklqjMEtSY0xuCWpMQa3JDXG4JakxhjcktQYg1uSGmNwS1JjDG5JasyEwZ3k1iS7k2zuqX05yY4kG7vHpT3bbkiyNclTSf5wthqXpGHVzxn3N4GLj1K/qapWdY97AJKcA1wBfKA75r8nmT9TzUqS+gjuqvpH4KU+X+8y4Paq2ldVv2Tsbu8XTKM/SdJhpjPH/bkkm7qplFO72lLguZ59tne1IyRZk2RDkg179uyZRhuSNFymGtw3A78HrAJ2An8x2ReoqrVVNVpVoyMjI1NsQ5KGz5SCu6qer6oDVXUQ+Ct+Ox2yA1jes+uyriZJmiFTCu4kS3pWPwEcuuJkHXBFkhOTnA2sBB6aXouSpF4LJtohyXeBC4EzkmwH/gS4MMkqoIBtwB8BVNXjSe4AngD2A9dV1YHZaV2ShtOEwV1VVx6lfMvb7H8jcON0mpIkjc9vTkpSYwxuSWqMwS1JjTG4JakxBrckNcbglnrs2/sCr/2/pzjwm32DbkUa14SXA0pz2evP/4Kdj/zvN9f37d3Dvtde4AOf+jLzF505wM6k8RncGmr7/+k19m5/4q3FZDDNSH1yqkSSGmNwS1JjDG5JaozBLUmNMbglqTEGtyQ1xuCWpMYY3JLUGINbkhpjcEtSYyYM7iTLk9yf5Ikkjyf5fFc/Lcl9SZ7pnk/t6kny9SRbk2xKcv5sD0KShkk/Z9z7gS9W1TnAauC6JOcA1wPrq2olsL5bB7iEsbu7rwTWADfPeNeSNMQmDO6q2llVP+2WXwO2AEuBy4Dbut1uAy7vli8DvlVjHgAWJVky451L0pCa1Bx3khXAecCDwOKq2tlt2gUs7paXAs/1HLa9qx3+WmuSbEiyYc+ePZNsW5oh4/wlwDp48Bg3IvWv7+BO8k7g+8AXqmpv77aqKqAm88ZVtbaqRqtqdGRkZDKHSjPmny19P6ec8btvLVbx/KYfDaYhqQ99BXeShYyF9neq6gdd+flDUyDd8+6uvgNY3nP4sq4mHXfmLTiBzD/yz9Lv3/fGALqR+tPPVSUBbgG2VNXXejatA67ulq8G7uqpf6a7umQ18GrPlIokaZr6uQPOh4GrgJ8l2djV/hj4M+COJNcAzwKf7rbdA1wKbAXeAD47ox1L0pCbMLir6ifAePdy+uhR9i/gumn2JUkah9+clKTGGNyS1BiDW5IaY3BLUmMMbklqjMEtSY0xuCWpMQa3JDXG4JakxhjcktQYg1uSGmNwa+jNW3DCEbU6eIA6eGAA3UgTM7g19M5cdfERtb3bn+CNF/7vALqRJmZwa+jNm7/wyGId9PZlOm4Z3JLUGINbkhpjcEtSYwxuSWpMPzcLXp7k/iRPJHk8yee7+peT7EiysXtc2nPMDUm2JnkqyR/O5gAkadj0c7Pg/cAXq+qnSd4FPJLkvm7bTVX1n3t3TnIOcAXwAeB3gH9I8s+ryotiJWkGTHjGXVU7q+qn3fJrwBZg6dscchlwe1Xtq6pfMna39wtmollJ0iTnuJOsAM4DHuxKn0uyKcmtSU7takuB53oO287bB70kaRL6Du4k7wS+D3yhqvYCNwO/B6wCdgJ/MZk3TrImyYYkG/bs2TOZQyVpqPUV3EkWMhba36mqHwBU1fNVdaCqDgJ/xW+nQ3YAy3sOX9bV3qKq1lbVaFWNjoyMTGcMkjRU+rmqJMAtwJaq+lpPfUnPbp8ANnfL64ArkpyY5GxgJfDQzLUsScOtn6tKPgxcBfwsycau9sfAlUlWAQVsA/4IoKoeT3IH8ARjV6Rc5xUlkjRzJgzuqvoJkKNsuudtjrkRuHEafUmSxuE3JyWpMQa3ht68BQuZt+DEI+r7f/X6ALqRJmZwa+idfPpy3rX0/UfUn9/0owF0I03M4NbQG7tw6khVdYw7kfpjcEtSYwxuSWqMwS1JjTG4JakxBrckNcbglqTGGNyS1BiDW5IaY3BLUmP6+bOuUpO+8Y1vcO+99/a176fOfzf/YvFb/17Jk09u4YZvfqKv41evXs2XvvSlSfcoTYXBrTnrscce48477+xr348s+TjvHVnJwRr7JzEvB3jxxV3ceee6vo6fN88fXnXsGNwS8OuDJ/HQS5ewd//pALxj/qss2P/XA+5KOjpPEyTGgvvl37yHA7WQA7WQvfvPYPPeDw+6LemoDG4J2L1vOYff6Gl/nTCYZqQJ9HOz4JOSPJTksSSPJ/lKVz87yYNJtib5XpITuvqJ3frWbvuK2R2CNH3vOfE5xm6f+lsnz39tMM1IE+jnjHsfcFFVnQusAi5Oshr4c+Cmqnof8DJwTbf/NcDLXf2mbj/puLb+4Uf5nZOe4ZR5L/PyS8/yq1ce5fff/X84+8xFg25NOkI/Nwsu4NA9nBZ2jwIuAv5tV78N+DJwM3BZtwzwt8B/S5Lyr9LrOPboU89y+t1/yoGDxX0bfs6+3+wnwEE/tjoO9XVVSZL5wCPA+4C/BH4OvFJV+7tdtgNLu+WlwHMAVbU/yavA6cAL473+rl27+OpXvzqlAUjj2bhxY9/7vv5Pv+bOn2x5S20ykf3000/7GdaM2rVr17jb+gruqjoArEqyCPghcOQN+iYpyRpgDcDSpUu56qqrpvuS0lts3ryZBx544Ji811lnneVnWDPq29/+9rjbJnUdd1W9kuR+4EPAoiQLurPuZcCObrcdwHJge5IFwLuBF4/yWmuBtQCjo6N15plnTqYVaUKnnHLKMXuvk046CT/DmkkLFy4cd1s/V5WMdGfaJDkZ+BiwBbgf+GS329XAXd3yum6dbvuPnd+WpJnTzxn3EuC2bp57HnBHVd2d5Ang9iR/CjwK3NLtfwvw10m2Ai8BV8xC35I0tPq5qmQTcN5R6r8ALjhK/VfAp2akO0nSEfzmpCQ1xuCWpMb41wE1Z5177rlcfvnlx+S9LrjgiFlDadYY3Jqzrr32Wq699tpBtyHNOKdKJKkxBrckNcbglqTGGNyS1BiDW5IaY3BLUmMMbklqjMEtSY0xuCWpMQa3JDXG4JakxhjcktQYg1uSGmNwS1Jj+rlZ8ElJHkryWJLHk3ylq38zyS+TbOweq7p6knw9ydYkm5KcP9uDkKRh0s/f494HXFRVrydZCPwkyd912/59Vf3tYftfAqzsHh8Ebu6eJUkzYMIz7hrzere6sHvU2xxyGfCt7rgHgEVJlky/VUkS9DnHnWR+ko3AbuC+qnqw23RjNx1yU5ITu9pS4Lmew7d3NUnSDOgruKvqQFWtApYBFyT5feAG4P3AvwJOA740mTdOsibJhiQb9uzZM8m2JWl4Teqqkqp6BbgfuLiqdnbTIfuA/wkculvqDmB5z2HLutrhr7W2qkaranRkZGRq3UvSEOrnqpKRJIu65ZOBjwFPHpq3ThLgcmBzd8g64DPd1SWrgVerauesdC9JQ6ifq0qWALclmc9Y0N9RVXcn+XGSESDARuDQ7bTvAS4FtgJvAJ+d+bYlaXhNGNxVtQk47yj1i8bZv4Drpt+aJOlo/OakJDXG4JakxhjcktQYg1uSGmNwS1JjDG5JaozBLUmNMbglqTEGtyQ1xuCWpMYY3JLUGINbkhpjcEtSYwxuSWqMwS1JjTG4JakxBrckNcbglqTGGNyS1BiDW5IaY3BLUmMMbklqTKpq0D2Q5DXgqUH3MUvOAF4YdBOzYK6OC+bu2BxXW363qkaOtmHBse5kHE9V1eigm5gNSTbMxbHN1XHB3B2b45o7nCqRpMYY3JLUmOMluNcOuoFZNFfHNlfHBXN3bI5rjjgufjkpSerf8XLGLUnq08CDO8nFSZ5KsjXJ9YPuZ7KS3Jpkd5LNPbXTktyX5Jnu+dSuniRf78a6Kcn5g+v87SVZnuT+JE8keTzJ57t602NLclKSh5I81o3rK1397CQPdv1/L8kJXf3Ebn1rt33FIPufSJL5SR5Ncne3PlfGtS3Jz5JsTLKhqzX9WZyOgQZ3kvnAXwKXAOcAVyY5Z5A9TcE3gYsPq10PrK+qlcD6bh3Gxrmye6wBbj5GPU7FfuCLVXUOsBq4rvtv0/rY9gEXVdW5wCrg4iSrgT8Hbqqq9wEvA9d0+18DvNzVb+r2O559HtjSsz5XxgXwB1W1qufSv9Y/i1NXVQN7AB8C7u1ZvwG4YZA9TXEcK4DNPetPAUu65SWMXacO8D+AK4+23/H+AO4CPjaXxgacAvwU+CBjX+BY0NXf/FwC9wIf6pYXdPtl0L2PM55ljAXYRcDdQObCuLoetwFnHFabM5/FyT4GPVWyFHiuZ317V2vd4qra2S3vAhZ3y02Ot/sx+jzgQebA2LrphI3AbuA+4OfAK1W1v9ult/c3x9VtfxU4/dh23Lf/AvwH4GC3fjpzY1wABfwoySNJ1nS15j+LU3W8fHNyzqqqStLspTtJ3gl8H/hCVe1N8ua2VsdWVQeAVUkWAT8E3j/glqYtyb8GdlfVI0kuHHQ/s+AjVbUjyXuA+5I82bux1c/iVA36jHsHsLxnfVlXa93zSZYAdM+7u3pT402ykLHQ/k5V/aArz4mxAVTVK8D9jE0hLEpy6ESmt/c3x9Vtfzfw4jFutR8fBv5Nkm3A7YxNl/xX2h8XAFW1o3vezdj/bC9gDn0WJ2vQwf0wsLL7zfcJwBXAugH3NBPWAVd3y1czNj98qP6Z7rfeq4FXe37UO65k7NT6FmBLVX2tZ1PTY0sy0p1pk+RkxubttzAW4J/sdjt8XIfG+0ngx9VNnB5PquqGqlpWVSsY+3f046r6dzQ+LoAk70jyrkPLwMeBzTT+WZyWQU+yA5cCTzM2z/gfB93PFPr/LrAT+A1jc2nXMDZXuB54BvgH4LRu3zB2Fc3PgZ8Bo4Pu/23G9RHG5hU3ARu7x6Wtjw34l8Cj3bg2A/+pq78XeAjYCvwv4MSuflK3vrXb/t5Bj6GPMV4I3D1XxtWN4bHu8fihnGj9szidh9+clKTGDHqqRJI0SQa3JDXG4JakxhjcktQYg1uSGmNwS1JjDG5JaozBLUmN+f/mP34rZ+GZ9QAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75eHkuwTi2Si"
      },
      "source": [
        "# Building the network for REINFORCE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_TFCmsWi2Sj"
      },
      "source": [
        "For REINFORCE algorithm, we'll need a model that predicts action probabilities given states.\n",
        "\n",
        "For numerical stability, please __do not include the softmax layer into your network architecture__.\n",
        "We'll use softmax or log-softmax where appropriate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sY2THBWfi2Sl"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8_pYr7PZi2Sn"
      },
      "source": [
        "# Build a simple neural network that predicts policy logits. \n",
        "# Keep it simple: CartPole isn't worth deep architectures.\n",
        "model = nn.Sequential(\n",
        "  nn.Linear(state_dim[0], 64),\n",
        "  nn.ReLU(),\n",
        "  nn.Linear(64, 16),\n",
        "  nn.ReLU(),\n",
        "  nn.Linear(16, n_actions),\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Y80qbQFi2Sq"
      },
      "source": [
        "#### Predict function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12PjRu0mi2Sr"
      },
      "source": [
        "Note: output value of this function is not a torch tensor, it's a numpy array.\n",
        "So, here gradient calculation is not needed.\n",
        "<br>\n",
        "Use [no_grad](https://pytorch.org/docs/stable/autograd.html#torch.autograd.no_grad)\n",
        "to suppress gradient calculation.\n",
        "<br>\n",
        "Also, `.detach()` (or legacy `.data` property) can be used instead, but there is a difference:\n",
        "<br>\n",
        "With `.detach()` computational graph is built but then disconnected from a particular tensor,\n",
        "so `.detach()` should be used if that graph is needed for backprop via some other (not detached) tensor;\n",
        "<br>\n",
        "In contrast, no graph is built by any operation in `no_grad()` context, thus it's preferable here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5B5JuXCi2St"
      },
      "source": [
        "def predict_probs(states):\n",
        "    \"\"\" \n",
        "    Predict action probabilities given states.\n",
        "    :param states: numpy array of shape [batch, state_shape]\n",
        "    :returns: numpy array of shape [batch, n_actions]\n",
        "    \"\"\"\n",
        "    # convert states, compute logits, use softmax to get probability\n",
        "    with torch.no_grad():\n",
        "      states_tensor = torch.tensor(states, dtype=torch.float32)\n",
        "      logits = model(states_tensor) # [batch, n_actions]\n",
        "      probs = torch.nn.Softmax()(logits)  # [batch, n_actions]\n",
        "\n",
        "    return probs.numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Obkl_jCii2Sv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "925125e9-769a-4c5c-e6bf-700904c28c44"
      },
      "source": [
        "test_states = np.array([env.reset() for _ in range(5)])\n",
        "test_probas = predict_probs(test_states)\n",
        "assert isinstance(test_probas, np.ndarray), \\\n",
        "    \"you must return np array and not %s\" % type(test_probas)\n",
        "assert tuple(test_probas.shape) == (test_states.shape[0], env.action_space.n), \\\n",
        "    \"wrong output shape: %s\" % np.shape(test_probas)\n",
        "assert np.allclose(np.sum(test_probas, axis=1), 1), \"probabilities do not sum to 1\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Be6AYf8gi2Sw"
      },
      "source": [
        "### Play the game\n",
        "\n",
        "We can now use our newly built agent to play the game."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8LOUUvnki2Sx"
      },
      "source": [
        "def generate_session(env, t_max=1000):\n",
        "    \"\"\" \n",
        "    Play a full session with REINFORCE agent.\n",
        "    Returns sequences of states, actions, and rewards.\n",
        "    \"\"\"\n",
        "    # arrays to record session\n",
        "    states, actions, rewards = [], [], []\n",
        "    s = env.reset()\n",
        "\n",
        "    for t in range(t_max):\n",
        "        # action probabilities array aka pi(a|s)\n",
        "        action_probs = predict_probs(np.array([s]))[0]\n",
        "\n",
        "        # Sample action with given probabilities.\n",
        "        a = np.random.choice(n_actions, p=action_probs)\n",
        "        new_s, r, done, info = env.step(a)\n",
        "\n",
        "        # record session history to train later\n",
        "        states.append(s)\n",
        "        actions.append(a)\n",
        "        rewards.append(r)\n",
        "\n",
        "        s = new_s\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    return states, actions, rewards"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5sdENWJAi2Sz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4ac6600-449f-4b85-aad9-451f2b0edd36"
      },
      "source": [
        "# test it\n",
        "states, actions, rewards = generate_session(env)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bN71dmtx5L-y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f60393e-e96c-4393-bfa3-9fe4f22cff9a"
      },
      "source": [
        "print(states)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[array([ 0.03889393, -0.01473188,  0.02180899,  0.02034458]), array([ 0.03859929, -0.21015969,  0.02221588,  0.31982793]), array([ 0.0343961 , -0.01536106,  0.02861244,  0.03423302]), array([ 0.03408888,  0.17933914,  0.0292971 , -0.24928684]), array([ 0.03767566,  0.37403072,  0.02431136, -0.53258677]), array([ 0.04515628,  0.56880245,  0.01365963, -0.8175112 ]), array([ 0.05653232,  0.3734962 , -0.0026906 , -0.52056335]), array([ 0.06400225,  0.56865592, -0.01310186, -0.81409291]), array([ 0.07537537,  0.37371583, -0.02938372, -0.5255597 ]), array([ 0.08284968,  0.17901942, -0.03989492, -0.24227872]), array([ 0.08643007, -0.01551063, -0.04474049,  0.03755817]), array([ 0.08611986,  0.1802234 , -0.04398933, -0.26889838]), array([ 0.08972433, -0.01424408, -0.04936729,  0.00959196]), array([ 0.08943945, -0.20862455, -0.04917545,  0.28629935]), array([ 0.08526696, -0.01283703, -0.04344947, -0.02147866]), array([ 0.08501021,  0.18288022, -0.04387904, -0.3275476 ]), array([ 0.08866782, -0.01159046, -0.05042999, -0.04901865]), array([ 0.08843601, -0.20595437, -0.05141037,  0.22733644]), array([ 0.08431692, -0.40030536, -0.04686364,  0.50336976]), array([ 0.07631082, -0.20455531, -0.03679624,  0.1962942 ]), array([ 0.07221971, -0.00892688, -0.03287036, -0.10776547]), array([ 0.07204117, -0.20356275, -0.03502567,  0.17436848]), array([ 0.06796992, -0.39816637, -0.0315383 ,  0.45579943]), array([ 0.06000659, -0.20261304, -0.02242231,  0.15334458]), array([ 0.05595433, -0.39740688, -0.01935542,  0.43887028]), array([ 0.04800619, -0.59224961, -0.01057801,  0.7253895 ]), array([ 0.0361612 , -0.39698299,  0.00392978,  0.42939608]), array([ 0.02822154, -0.59216038,  0.0125177 ,  0.72331525]), array([ 0.01637833, -0.7874532 ,  0.026984  ,  1.01991164]), array([ 6.29266781e-04, -9.82924162e-01,  4.73822377e-02,  1.32094368e+00]), array([-0.01902922, -1.17861188,  0.07380111,  1.62807098]), array([-0.04260145, -1.3745198 ,  0.10636253,  1.94281078]), array([-0.07009185, -1.18068068,  0.14521875,  1.68490688]), array([-0.09370546, -0.98750741,  0.17891688,  1.44074133]), array([-0.11345561, -1.18432438,  0.20773171,  1.78357548])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eG5hLg-3i2S0"
      },
      "source": [
        "### Computing cumulative rewards\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "G_t &= r_t + \\gamma r_{t + 1} + \\gamma^2 r_{t + 2} + \\ldots \\\\\n",
        "&= \\sum_{i = t}^T \\gamma^{i - t} r_i \\\\\n",
        "&= r_t + \\gamma * G_{t + 1}\n",
        "\\end{align*}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AoWX9gvai2S0"
      },
      "source": [
        "def get_cumulative_rewards(rewards,  # rewards at each step\n",
        "                           gamma=0.99  # discount for reward\n",
        "                           ):\n",
        "    \"\"\"\n",
        "    Take a list of immediate rewards r(s,a) for the whole session \n",
        "    and compute cumulative returns (a.k.a. G(s,a) in Sutton '16).\n",
        "    \n",
        "    G_t = r_t + gamma*r_{t+1} + gamma^2*r_{t+2} + ...\n",
        "\n",
        "    A simple way to compute cumulative rewards is to iterate from the last\n",
        "    to the first timestep and compute G_t = r_t + gamma*G_{t+1} recurrently\n",
        "\n",
        "    You must return an array/list of cumulative rewards with as many elements as in the initial rewards.\n",
        "    \"\"\"\n",
        "    cumulative_rewards = np.empty_like(rewards, dtype=np.float32)\n",
        "    total = 0\n",
        "    for i in range(len(rewards) - 1, -1, -1):\n",
        "      total = rewards[i] + gamma * total\n",
        "      cumulative_rewards[i] = total\n",
        "\n",
        "    return cumulative_rewards"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2DX39wcUi2S3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4e003c9-5c4a-439e-ee87-c15749331cb6"
      },
      "source": [
        "get_cumulative_rewards(rewards)\n",
        "assert len(get_cumulative_rewards(list(range(100)))) == 100\n",
        "assert np.allclose(\n",
        "    get_cumulative_rewards([0, 0, 1, 0, 0, 1, 0], gamma=0.9),\n",
        "    [1.40049, 1.5561, 1.729, 0.81, 0.9, 1.0, 0.0])\n",
        "assert np.allclose(\n",
        "    get_cumulative_rewards([0, 0, 1, -2, 3, -4, 0], gamma=0.5),\n",
        "    [0.0625, 0.125, 0.25, -1.5, 1.0, -4.0, 0.0])\n",
        "assert np.allclose(\n",
        "    get_cumulative_rewards([0, 0, 1, 2, 3, 4, 0], gamma=0),\n",
        "    [0, 0, 1, 2, 3, 4, 0])\n",
        "print(\"looks good!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "looks good!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evLt5DJji2S_"
      },
      "source": [
        "#### Loss function and updates\n",
        "\n",
        "We now need to define objective and update over policy gradient.\n",
        "\n",
        "Our objective function is\n",
        "\n",
        "$$ J \\approx  { 1 \\over N } \\sum_{s_i,a_i} G(s_i,a_i) $$\n",
        "\n",
        "REINFORCE defines a way to compute the gradient of the expected reward with respect to policy parameters. The formula is as follows:\n",
        "\n",
        "$$ \\nabla_\\theta \\hat J(\\theta) \\approx { 1 \\over N } \\sum_{s_i, a_i} \\nabla_\\theta \\log \\pi_\\theta (a_i \\mid s_i) \\cdot G_t(s_i, a_i) $$\n",
        "\n",
        "We can abuse PyTorch's capabilities for automatic differentiation by defining our objective function as follows:\n",
        "\n",
        "$$ \\hat J(\\theta) \\approx { 1 \\over N } \\sum_{s_i, a_i} \\log \\pi_\\theta (a_i \\mid s_i) \\cdot G_t(s_i, a_i) $$\n",
        "\n",
        "When you compute the gradient of that function with respect to network weights $\\theta$, it will become exactly the policy gradient."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_hLjxTVLi2TB"
      },
      "source": [
        "def to_one_hot(y_tensor, ndims):\n",
        "    \"\"\" helper: take an integer vector and convert it to 1-hot matrix. \"\"\"\n",
        "    y_tensor = y_tensor.type(torch.LongTensor).view(-1, 1)\n",
        "    y_one_hot = torch.zeros(\n",
        "        y_tensor.size()[0], ndims).scatter_(1, y_tensor, 1)\n",
        "    return y_one_hot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1C8ZSizji2TD"
      },
      "source": [
        "# Your code: define optimizers\n",
        "optimizer = torch.optim.Adam(model.parameters(), 1e-3)\n",
        "\n",
        "\n",
        "def train_on_session(states, actions, rewards, gamma=0.99, entropy_coef=1e-2):\n",
        "    \"\"\"\n",
        "    Takes a sequence of states, actions and rewards produced by generate_session.\n",
        "    Updates agent's weights by following the policy gradient above.\n",
        "    Please use Adam optimizer with default parameters.\n",
        "    \"\"\"\n",
        "\n",
        "    # cast everything into torch tensors\n",
        "    states = torch.tensor(states, dtype=torch.float32)\n",
        "    actions = torch.tensor(actions, dtype=torch.int32)\n",
        "    cumulative_returns = np.array(get_cumulative_rewards(rewards, gamma))\n",
        "    cumulative_returns = torch.tensor(cumulative_returns, dtype=torch.float32)\n",
        "\n",
        "    # predict logits, probas and log-probas using an agent.\n",
        "    logits = model(states)\n",
        "    probs = nn.functional.softmax(logits, -1)\n",
        "    log_probs = nn.functional.log_softmax(logits, -1)\n",
        "\n",
        "    assert all(isinstance(v, torch.Tensor) for v in [logits, probs, log_probs]), \\\n",
        "        \"please use compute using torch tensors and don't use predict_probs function\"\n",
        "\n",
        "    # select log-probabilities for chosen actions, log pi(a_i|s_i)\n",
        "    log_probs_for_actions = torch.sum(\n",
        "        log_probs * to_one_hot(actions, env.action_space.n), dim=1) # [batch,]\n",
        "    \n",
        "    J_hat = torch.mean(log_probs_for_actions * cumulative_returns)  # a number\n",
        "   \n",
        "    # Compute loss here. Don't forgen entropy regularization with `entropy_coef` \n",
        "    entropy = - (probs * log_probs).sum(1).mean()\n",
        "    loss = -J_hat - entropy_coef * entropy\n",
        "\n",
        "    # Gradient descent step\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # technical: return session rewards to print them later\n",
        "    return np.sum(rewards)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-WWsbl5i2TE"
      },
      "source": [
        "### The actual training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ckHj5sXBi2TE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0c7b3f7-97b8-47f1-f195-4c77935ef65f"
      },
      "source": [
        "for i in range(100):\n",
        "    rewards = [train_on_session(*generate_session(env)) for _ in range(100)]  # generate new sessions\n",
        "    \n",
        "    print(\"mean reward:%.3f\" % (np.mean(rewards)))\n",
        "    \n",
        "    if np.mean(rewards) > 500:\n",
        "        print(\"You Win!\")  # but you can train even further\n",
        "        break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mean reward:28.880\n",
            "mean reward:27.970\n",
            "mean reward:45.630\n",
            "mean reward:65.820\n",
            "mean reward:69.840\n",
            "mean reward:175.300\n",
            "mean reward:171.190\n",
            "mean reward:144.710\n",
            "mean reward:230.720\n",
            "mean reward:156.020\n",
            "mean reward:145.040\n",
            "mean reward:484.620\n",
            "mean reward:259.760\n",
            "mean reward:227.660\n",
            "mean reward:252.230\n",
            "mean reward:540.100\n",
            "You Win!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bg__sQeti2TF"
      },
      "source": [
        "### Results & video"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cAoTsq4Pi2TF"
      },
      "source": [
        "# Record sessions\n",
        "\n",
        "import gym.wrappers\n",
        "\n",
        "with gym.wrappers.Monitor(gym.make(\"CartPole-v0\"), directory=\"videos\", force=True) as env_monitor:\n",
        "    sessions = [generate_session(env_monitor) for _ in range(100)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IgcqBKtBi2TG"
      },
      "source": [
        "# Show video. This may not work in some setups. If it doesn't\n",
        "# work for you, you can download the videos and view them locally.\n",
        "\n",
        "from pathlib import Path\n",
        "from base64 import b64encode\n",
        "from IPython.display import HTML\n",
        "\n",
        "video_paths = sorted([s for s in Path('videos').iterdir() if s.suffix == '.mp4'])\n",
        "video_path = video_paths[-1]  # You can also try other indices\n",
        "\n",
        "if 'google.colab' in sys.modules:\n",
        "    # https://stackoverflow.com/a/57378660/1214547\n",
        "    with video_path.open('rb') as fp:\n",
        "        mp4 = fp.read()\n",
        "    data_url = 'data:video/mp4;base64,' + b64encode(mp4).decode()\n",
        "else:\n",
        "    data_url = str(video_path)\n",
        "\n",
        "HTML(\"\"\"\n",
        "<video width=\"640\" height=\"480\" controls>\n",
        "  <source src=\"{}\" type=\"video/mp4\">\n",
        "</video>\n",
        "\"\"\".format(data_url))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}